{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as tf\n",
    "import DL_utils as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H, n_W, n_C, n_y):\n",
    "    '''\n",
    "    Function to create placeholder for tensorflow session\n",
    "    \n",
    "    Args:\n",
    "    n_H = height of the image\n",
    "    n_W = width of image\n",
    "    n_C = number of channels\n",
    "    n_y = number of output features\n",
    "    \n",
    "    returns:\n",
    "    X,Y\n",
    "    '''\n",
    "    X = tf.placeholder(tf.float32, shape = (None, n_H, n_W, n_C))\n",
    "    Y = tf.placeholder(tf.float32, shape = (None, n_y))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_forward_propagation(X, parameters):\n",
    "    '''\n",
    "    Forward propagation for AlexNet without normalization\n",
    "    \n",
    "    '''\n",
    "    # 1st conv\n",
    "    A1 = dl.conv_layer(X,parameters['W1'],parameters['b1'], stride=[1,1,1,1], padding='SAME', name='1-1')\n",
    "    # 2nd conv\n",
    "    A2 = dl.conv_layer(X,parameters['W2'],parameters['b2'], stride=[1,1,1,1], padding='SAME', name='1-2')\n",
    "    # 1st pool\n",
    "    P1 = dl.max_pool(A2, kernel=[1,2,2,1], strides=[1,2,2,1], padding = 'VALID', name='1')\n",
    "    # 3rd conv\n",
    "    A3 = dl.conv_layer(P1,parameters['W3'],parameters['b3'], strides=[1,1,1,1], padding='SAME', name='2-1')\n",
    "    # 4th conv\n",
    "    A4 = dl.conv_layer(A3,parameters['W4'],parameters['b4'], strides=[1,1,1,1], padding='SAME', name='2-2')\n",
    "    #2nd pool\n",
    "    P2 = dl.max_pool(A4, kernel=[1,2,2,1], strides=[1,2,2,1], padding = 'VALID', name='2')\n",
    "    # 5th conv\n",
    "    A5 = dl.conv_layer(P2,parameters['W5'], parameters['b5'], strides=[1,1,1,1], padding='SAME', name='3-1')\n",
    "    # 6th conv\n",
    "    A6 = dl.conv_layer(A5,parameters['W6'], parameters['b6'], strides=[1,1,1,1], padding='SAME', name='3-2')\n",
    "    # 3rd pool\n",
    "    P3 = dl.max_pool(A6, kernel=[1,2,2,1], strides=[1,2,2,1], padding = 'VALID', name='3')\n",
    "    # 7th conv\n",
    "    A7 = dl.conv_layer(P3,parameters['W7'], parameters['b7'], strides=[1,1,1,1], padding='SAME', name='4-1')\n",
    "    # 8th conv\n",
    "    A8 = dl.conv_layer(A7,parameters['W8'], parameters['b8'], strides=[1,1,1,1], padding='SAME', name='4-2')\n",
    "    # 9th conv\n",
    "    A9 = dl.conv_layer(A8,parameters['W9'], parameters['b9'], strides=[1,1,1,1], padding='SAME', name='4-3')\n",
    "    # 4th pool\n",
    "    P4 = dl.max_pool(A9, kernel=[1,2,2,1], strides=[1,2,2,1], padding = 'VALID', name='4')\n",
    "    # 10th conv\n",
    "    A10 = dl.conv_layer(P4,parameters['W10'], parameters['b10'], strides=[1,1,1,1], padding='SAME', name='5-1')\n",
    "    # 11th conv\n",
    "    A11 = dl.conv_layer(A10,parameters['W11'], parameters['b11'], strides=[1,1,1,1], padding='SAME', name='5-2')\n",
    "    # 12th conv\n",
    "    A12 = dl.conv_layer(A11,parameters['W12'], parameters['b12'], strides=[1,1,1,1], padding='SAME', name='5-3')\n",
    "    # 5th pool\n",
    "    P5 = dl.max_pool(A12, kernel=[1,2,2,1], strides=[1,2,2,1], padding = 'VALID', name='5')\n",
    "    # Flattening the last pooling layer\n",
    "    P5 = tf.contrib.layers.flatten(P5)\n",
    "    # FC1 - 4096 neurons\n",
    "    F1 = dl.fc_layer(P5, 4096, activation_fn=None, name='1')\n",
    "    # FC2 - 4096 neurons\n",
    "    F2 = dl.fc_layer(F1, 4096, activation_fn=None, name='2')\n",
    "    # FC3 - 1000 neurons reping different classes - may be modified for other models\n",
    "    F3 = dl.fc_layer(F2, 1000, activation_fn=None, name='3')\n",
    "    \n",
    "    return F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AN_cost(Z, Y):\n",
    "    '''\n",
    "    Computes cost \n",
    "    \n",
    "    Args:\n",
    "    Z8 -- Logits/Linear output from the last fully connected layer\n",
    "    Y -- labels corresponding to each example in the batch\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cost of the epoch\n",
    "    '''\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    '''\n",
    "    Implements the AlexNet model for image classification\n",
    "    \n",
    "    Args:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    '''\n",
    "    # retrieve necessary tensor dimensions\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    seed = 3\n",
    "    \n",
    "    # Create placeholders for the input and output tensors\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    \n",
    "    # create dictionary to assign tensor shapes to the weight filters for AlexNet\n",
    "    parameter_size_dict = {\"W1\":[11,11,3,96], \"W2\":[5,5,96,256], \"W3\":[3,3,256,384], \"W4\":[3,3,384,384], \"W5\":[3,3,384,256]}\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(parameter_size_dict)\n",
    "    \n",
    "    # apply forward propogation\n",
    "    Z = forward_propogation(X, parameters)\n",
    "    \n",
    "    # compute cost \n",
    "    cost = compute_cost(Z, Y)\n",
    "    \n",
    "    # define an optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # initialize global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # run the session and train\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # run the session\n",
    "        sess.run(init)\n",
    "        \n",
    "        # training loop\n",
    "        for epoch in num_epochs:\n",
    "            \n",
    "            # split into minibatches\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                # select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # computation of the actual graph\n",
    "                _, temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                # add the cost\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "                \n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the correct predictions\n",
    "    predict_op = tf.argmax(Z3, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(accuracy)\n",
    "    train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "    test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "    return train_accuracy, test_accuracy, parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
