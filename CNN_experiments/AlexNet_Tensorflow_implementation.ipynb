{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H, n_W, n_C, n_y):\n",
    "    '''\n",
    "    Function to create placeholder for tensorflow session\n",
    "    \n",
    "    Args:\n",
    "    n_H = height of the image\n",
    "    n_W = width of image\n",
    "    n_C = number of channels\n",
    "    n_y = number of output features\n",
    "    \n",
    "    returns:\n",
    "    X,Y\n",
    "    '''\n",
    "    X = tf.placeholder(tf.float32, shape = (None, n_H, n_W, n_C))\n",
    "    Y = tf.placeholder(tf.float32, shape = (None, n_y))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      "Y = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# unit test script\n",
    "X, Y = create_placeholders(64, 64, 3, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, (k * mini_batch_size) : ((k + 1) * mini_batch_size)]\n",
    "        mini_batch_Y = shuffled_Y[:, (k * mini_batch_size) : ((k + 1) * mini_batch_size)]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, (num_complete_minibatches * mini_batch_size) : ]\n",
    "        mini_batch_Y = shuffled_Y[:, (num_complete_minibatches * mini_batch_size) : ]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(parameter_size_dict):\n",
    "    '''\n",
    "    Function to initialize weight parameters, which in the case of AlexNet are filters of varying sizes. \n",
    "    They are Initialized using the xavier initializer\n",
    "    \n",
    "    Args:\n",
    "    parameter_size_dict: dictonary carrying the sizes of each parameter as a list\n",
    "    \n",
    "    returns:\n",
    "    parameters containing initialized weights\n",
    "    '''\n",
    "    tf.set_random_seed(1)\n",
    "    W1 = tf.get_variable(\"W1\", parameter_size_dict[\"W1\"], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable(\"W2\", parameter_size_dict[\"W2\"], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W3 = tf.get_variable(\"W3\", parameter_size_dict[\"W3\"], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W4 = tf.get_variable(\"W4\", parameter_size_dict[\"W4\"], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W5 = tf.get_variable(\"W5\", parameter_size_dict[\"W5\"], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    \n",
    "    \n",
    "    parameters = {\"W1\":W1,\n",
    "                 \"W2\":W2,\n",
    "                 \"W3\":W3,\n",
    "                 \"W4\":W4,\n",
    "                 \"W5\":W5,\n",
    "                 }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394\n",
      " -0.06847463  0.05245192]\n",
      "W2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
      " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
      " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n"
     ]
    }
   ],
   "source": [
    "# unit test script\n",
    "parameter_size_dict = {\"W1\":[4,4,3,8], \"W2\":[2,2,8,16], \"W3\":[2,2,8,16], \"W4\":[3,3,8,16], \"W5\":[3,3,4,12]}\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters(parameter_size_dict)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"].eval()[1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(X, parameters):\n",
    "    '''\n",
    "    Function implements the forward propogation for the AlexNet.\n",
    "    The CNN architecture is as follows\n",
    "    Conv2D->ReLU->MaxPool->Conv2D->ReLU->MaxPool->Conv2D->ReLU->Conv2D->ReLU->Conv2D->ReLU->MaxPool->FC->FC->SoftMax\n",
    "    \n",
    "    Args:\n",
    "    X - placeholder for the initial feature tensor\n",
    "    parameters - dictionary containing filters\n",
    "    \n",
    "    returns\n",
    "    Z8 - output of the last LINEAR layer\n",
    "    \n",
    "    NOT IMPLEMENTED: NORMALIZATION\n",
    "    '''\n",
    "    # retrieve parameters\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    W5 = parameters['W5']\n",
    "    \n",
    "    # Code for 1st convolutional unit\n",
    "    # Conv2d unit -- filter:11x11, stride:4x4, no padding\n",
    "    # input channels -- 3\n",
    "    # output channels -- 96\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,4,4,1], padding = 'VALID')\n",
    "    # ReLU activation\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # Maxpool -- filter:3x3, stride:2x2\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    \n",
    "    # Code for 2nd convolutional unit\n",
    "    # Conv2d unit -- filter:5x5, stride:1, pad:2\n",
    "    # input channels -- 96\n",
    "    # output channels -- 256\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # ReLU activation\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # Maxpool -- filter:3x3, stride:2\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    # Code for successive continuous convolutional units\n",
    "    # Conv2d unit -- filter:3x3, stride:1, pad:1\n",
    "    # input channels -- 256\n",
    "    # output channels -- 384\n",
    "    Z3 = tf.nn.conv2d(P2,W3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # ReLU activation\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    # Conv2d unit -- filter:3x3, stride:1, pad:1\n",
    "    # input channels -- 384\n",
    "    # output channels -- 384\n",
    "    Z4 = tf.nn.conv2d(A3,W4, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # ReLU activation\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    # Conv2d unit -- filter:3x3, stride:1, pad:1\n",
    "    # input channels -- 384\n",
    "    # output channels -- 256\n",
    "    Z5 = tf.nn.conv2d(A4,W5, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # ReLU activation\n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    # Maxpool -- filter:3x3, stride:2\n",
    "    P5 = tf.nn.max_pool(A5, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    # Code for Fully connected layers\n",
    "    # FC1 - 4096 neurons\n",
    "    P5 = tf.contrib.layers.flatten(P5)\n",
    "    Z6 = tf.contrib.layers.fully_connected(P5, 4096, activation_fn=None)\n",
    "    # FC2 - 4096 neurons\n",
    "    Z7 = tf.contrib.layers.fully_connected(Z6, 4096, activation_fn=None)\n",
    "    # FC3 - 1000 neurons reping different classes - may be modified for other models\n",
    "    Z8 = tf.contrib.layers.fully_connected(Z7, 1000, activation_fn=None)\n",
    "    \n",
    "    return Z8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test script to test forward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    '''\n",
    "    Computes cost \n",
    "    \n",
    "    Args:\n",
    "    Z8 -- Logits/Linear output from the last fully connected layer\n",
    "    Y -- labels corresponding to each example in the batch\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cost of the epoch\n",
    "    '''\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y))\n",
    "    \n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test script to test computation of cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    '''\n",
    "    Implements the AlexNet model for image classification\n",
    "    \n",
    "    Args:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    '''\n",
    "    # retrieve necessary tensor dimensions\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    seed = 3\n",
    "    \n",
    "    # Create placeholders for the input and output tensors\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    \n",
    "    # create dictionary to assign tensor shapes to the weight filters for AlexNet\n",
    "    parameter_size_dict = {\"W1\":[11,11,3,96], \"W2\":[5,5,96,256], \"W3\":[3,3,256,384], \"W4\":[3,3,384,384], \"W5\":[3,3,384,256]}\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(parameter_size_dict)\n",
    "    \n",
    "    # apply forward propogation\n",
    "    Z = forward_propogation(X, parameters)\n",
    "    \n",
    "    # compute cost \n",
    "    cost = compute_cost(Z, Y)\n",
    "    \n",
    "    # define an optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # initialize global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # run the session and train\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # run the session\n",
    "        sess.run(init)\n",
    "        \n",
    "        # training loop\n",
    "        for epoch in num_epochs:\n",
    "            \n",
    "            # split into minibatches\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                # select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # computation of the actual graph\n",
    "                _, temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                # add the cost\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "                \n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the correct predictions\n",
    "    predict_op = tf.argmax(Z3, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(accuracy)\n",
    "    train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "    test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "    return train_accuracy, test_accuracy, parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
