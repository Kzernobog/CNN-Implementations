{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv1 Implementation\n",
    "This notebook aims to implement the YOLOv1 object detection algorithm and replicate the results as given in [this](https://arxiv.org/abs/1506.02640) paper.\n",
    "\n",
    "Steps involved:\n",
    "- pre-training weights on the ImageNet dataset.\n",
    "- Implement the YOLOv1 model\n",
    "\n",
    "## Step 1\n",
    "Pre-training weights in ImageNet dataset\n",
    "- prepare the modified network model\n",
    "- prepare the dataset for training - done in an accompanying notebook\n",
    "- Implement the diagnostic functions to track training\n",
    "- train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib as plt\n",
    "from yolo_utils import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders_pretrain(n_H, n_W, n_C, n_y):\n",
    "    '''\n",
    "    Function to create placeholder for the input tensors\n",
    "    \n",
    "    Args:\n",
    "    n_H = height of the image tensor\n",
    "    n_W = width of the image tensor\n",
    "    n_C = number of channels in the image tensor\n",
    "    n_y = number of classes/output features\n",
    "    \n",
    "    returns:\n",
    "    X, Y\n",
    "    '''\n",
    "    X = tf.placeholder(tf.float32, shape = (None, n_H, n_W, n_C))\n",
    "    Y = tf.placeholder(tf.float32, shape = (None, n_y))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test script: DELETE\n",
    "c = [448, 448, 3, 10]\n",
    "X, Y = create_placeholders(*c)\n",
    "print(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that initializes weight parameters based on the configuration given in an xml file\n",
    "path_to_xml = './YOLOv1_Pre_trained_Model.xml'\n",
    "pre_train_parameters = initialize_weights(path_to_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Prepare the modified network model for pre-trianing on ImagenNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_pretrain(X, pre_train_parameters):\n",
    "    '''\n",
    "    Args:\n",
    "    X - placeholder for the initial feature tensor\n",
    "    pre_train_parameters - dictionary containing filters\n",
    "    \n",
    "    returns\n",
    "    FC1 - output of the last fully connected layer\n",
    "    \n",
    "    NOT IMPLEMENTED: NORMALIZATION\n",
    "    '''\n",
    "    # Conv\n",
    "    Z1 = tf.nn.conv2d(X, pre_train_parameters['W01'], [1,2,2,1], padding=\"SAME\")\n",
    "    Z1 = tf.nn.bias_add(Z1, pre_train_parameters['B01'])\n",
    "    A1 = tf.nn.leaky_relu(Z1, alpha=0.1)\n",
    "    \n",
    "    # Pool\n",
    "    P1 = tf.nn.max_pool(A1, [1,2,2,1], [1,2,2,1], padding=\"SAME\")\n",
    "    \n",
    "    # Conv\n",
    "    Z2 = tf.nn.conv2d(P1, pre_train_parameters['W02'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z2 = tf.nn.bias_add(Z2, pre_train_parameters['B02'])\n",
    "    A2 = tf.nn.leaky_relu(Z2, alpha=0.1)\n",
    "    \n",
    "    # Pool\n",
    "    P2 = tf.nn.max_pool(A2, [1,2,2,1], [1,2,2,1], padding=\"SAME\")\n",
    "    \n",
    "    # Conv\n",
    "    Z3 = tf.nn.conv2d(P2, pre_train_parameters['W03'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z3 = tf.nn.bias_add(Z3, pre_train_parameters['B03'])\n",
    "    A3 = tf.nn.leaky_relu(Z3, alpha=0.1)\n",
    "    Z4 = tf.nn.conv2d(A3, pre_train_parameters['W04'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z4 = tf.nn.bias_add(Z4, pre_train_parameters['B04'])\n",
    "    A4 = tf.nn.leaky_relu(Z4, alpha=0.1)\n",
    "    Z5 = tf.nn.conv2d(A4, pre_train_parameters['W05'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z5 = tf.nn.bias_add(Z5, pre_train_parameters['B05'])\n",
    "    A5 = tf.nn.leaky_relu(Z5, alpha=0.1)\n",
    "    Z6 = tf.nn.conv2d(A5, pre_train_parameters['W06'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z6 = tf.nn.bias_add(Z6, pre_train_parameters['B06'])\n",
    "    A6 = tf.nn.leaky_relu(Z6, alpha=0.1)\n",
    "    \n",
    "    # Pool\n",
    "    P3 = tf.nn.max_pool(A6, [1,2,2,1], [1,2,2,1], padding=\"SAME\")\n",
    "    \n",
    "    # Conv\n",
    "    Z7 = tf.nn.conv2d(P3, pre_train_parameters['W07'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z7 = tf.nn.bias_add(Z7, pre_train_parameters['B07'])\n",
    "    A7 = tf.nn.leaky_relu(Z7, alpha=0.1)\n",
    "    Z8 = tf.nn.conv2d(A7, pre_train_parameters['W08'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z8 = tf.nn.bias_add(Z8, pre_train_parameters['B08'])\n",
    "    A8 = tf.nn.leaky_relu(Z8, alpha=0.1)\n",
    "    Z9 = tf.nn.conv2d(A8, pre_train_parameters['W09'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z9 = tf.nn.bias_add(Z9, pre_train_parameters['B09'])\n",
    "    A9 = tf.nn.leaky_relu(Z9, alpha=0.1)\n",
    "    Z10 = tf.nn.conv2d(A9, pre_train_parameters['W10'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z10 = tf.nn.bias_add(Z10, pre_train_parameters['B10'])\n",
    "    A10 = tf.nn.leaky_relu(Z10, alpha=0.1)\n",
    "    Z11 = tf.nn.conv2d(A10, pre_train_parameters['W11'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z11 = tf.nn.bias_add(Z11, pre_train_parameters['B11'])\n",
    "    A11 = tf.nn.leaky_relu(Z11, alpha=0.1)\n",
    "    Z12 = tf.nn.conv2d(A11, pre_train_parameters['W12'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z12 = tf.nn.bias_add(Z12, pre_train_parameters['B12'])\n",
    "    A12 = tf.nn.leaky_relu(Z12, alpha=0.1)\n",
    "    Z13 = tf.nn.conv2d(A12, pre_train_parameters['W13'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z13 = tf.nn.bias_add(Z13, pre_train_parameters['B13'])\n",
    "    A13 = tf.nn.leaky_relu(Z13, alpha=0.1)\n",
    "    Z14 = tf.nn.conv2d(A13, pre_train_parameters['W14'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z14 = tf.nn.bias_add(Z14, pre_train_parameters['B14'])\n",
    "    A14 = tf.nn.leaky_relu(Z14, alpha=0.1)\n",
    "    Z15 = tf.nn.conv2d(A14, pre_train_parameters['W15'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z15 = tf.nn.bias_add(Z15, pre_train_parameters['B15'])\n",
    "    A15 = tf.nn.leaky_relu(Z15, alpha=0.1)\n",
    "    Z16 = tf.nn.conv2d(A15, pre_train_parameters['W16'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z16 = tf.nn.bias_add(Z16, pre_train_parameters['B16'])\n",
    "    A16 = tf.nn.leaky_relu(Z16, alpha=0.1)\n",
    "    \n",
    "    # Pool\n",
    "    P4 = tf.nn.max_pool(A16, [1,2,2,1], [1,2,2,1], padding=\"SAME\")\n",
    "    \n",
    "    \n",
    "    # Conv\n",
    "    Z17 = tf.nn.conv2d(P4, pre_train_parameters['W17'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z17 = tf.nn.bias_add(Z17, pre_train_parameters['B17'])\n",
    "    A17 = tf.nn.leaky_relu(Z17, alpha=0.1)\n",
    "    Z18 = tf.nn.conv2d(A17, pre_train_parameters['W18'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z18 = tf.nn.bias_add(Z18, pre_train_parameters['B18'])\n",
    "    A18 = tf.nn.leaky_relu(Z18, alpha=0.1)\n",
    "    Z19 = tf.nn.conv2d(A18, pre_train_parameters['W19'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z19 = tf.nn.bias_add(Z19, pre_train_parameters['B19'])\n",
    "    A19 = tf.nn.leaky_relu(Z19, alpha=0.1)\n",
    "    Z20 = tf.nn.conv2d(A19, pre_train_parameters['W20'], [1,1,1,1], padding=\"SAME\")\n",
    "    Z20 = tf.nn.bias_add(Z20, pre_train_parameters['B20'])\n",
    "    \n",
    "    # Pool\n",
    "    P5 = tf.nn.avg_pool(Z20, [1,2,2,1], [1,2,2,1], padding=\"SAME\")\n",
    "    \n",
    "    # flatten\n",
    "    P5 = tf.contrib.layers.flatten(P5)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    FC1 = tf.contrib.layers.fully_connected(P5, 1000, activation_fn=None)\n",
    "    \n",
    "    return FC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test script: DELETE\n",
    "tf.reset_default_graph()\n",
    "path_to_xml = './YOLOv1_Pre_trained_Model.xml'\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders_pretrain(448,448,3,1000)\n",
    "    pre_train_parameters = initialize_weights(path_to_xml)\n",
    "    FC1 = forward_propagation_pretrain(X, pre_train_parameters)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(FC1, {X: np.random.randn(2,448,448,3)})\n",
    "    print(\"Z20 = \" + str(a), \"Z20 shape = \" + str(a.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_pretrain(FC1, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=FC1, labels=Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ImageNet dataset has been created in batches in an accompanying notbook. Each batch contains 2000 images and their corresponding labels. This is the main reason for the additional for loop in the ensuing cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_folder = './ImageNet_dataset/training_folder'\n",
    "h5_files_list = os.listdir(path_to_train_folder)\n",
    "print(h5_files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below I implement the model function, which aggregates all the functions above, to train the model. I explicitly do not pass the train tensors as we are going to be reading them from another folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_test, Y_test, learning_rate = 0.09, num_epochs = 100, \n",
    "          minibatch_size = 64, print_cost = True, xml_path):\n",
    "    \n",
    "    # restting the default graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # retrieve image shapes\n",
    "    (m, n_Htr, n_Wtr, n_Ctr) = X_train.shape\n",
    "    n_Y = Y_train.shape\n",
    "    \n",
    "    # global variables\n",
    "    costs = []\n",
    "    \n",
    "    # randomizer\n",
    "    seed = int(np.random.randint(1,100,1))\n",
    "    \n",
    "    # creating placeholders \n",
    "    X, Y = create_placeholders_pretrain(448,448,3,1000)\n",
    "    \n",
    "    # initializing parameters\n",
    "    pretrain_parameters = initialize_weights(xml_path)\n",
    "    \n",
    "    # forward prop\n",
    "    FC1 = forward_propagation_pretrain(X, pretrain_parameters)\n",
    "    \n",
    "    # compute cost\n",
    "    cost = compute_cost_pretrain(FC1, Y)\n",
    "    \n",
    "    # select the appropriate the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # initialize global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # train the session\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # run the initialization for the session\n",
    "        sess.run(init)\n",
    "        \n",
    "        # for loop for epoch/iterations\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # maintain the cost through an epoch\n",
    "            epoch_cost = 0\n",
    "            \n",
    "            # randomizer\n",
    "            seed += 1\n",
    "            \n",
    "            # path to training folder\n",
    "            PATH = './ImageNet_dataset/training_folder/'\n",
    "            \n",
    "            # set up the data\n",
    "            h5_files = os.listdir(PATH)\n",
    "            \n",
    "            # for loop to iterate through the h5 files\n",
    "            for file in h5_files:\n",
    "                \n",
    "                # open the h5 file to form tensor\n",
    "                with h5py.File(PATH+file, mode = 'r') as h5_file:\n",
    "                    \n",
    "                    # extract features and labels\n",
    "                    X_train = np.asarray(h5_file['X_train'])\n",
    "                    Y_train = np.asarray(h5_file['Y_train'])\n",
    "                    \n",
    "                    # number of examples\n",
    "                    m = h5_file['X_train'].shape[0]\n",
    "                    \n",
    "                    # ??? - REASON WHY\n",
    "                    num_minibatches = int(m/minibatch_size)\n",
    "\n",
    "                    # generate minibatches\n",
    "                    minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "                    # iterate through the minibatches\n",
    "                    for minibatch in minibatches:\n",
    "                        \n",
    "                        # procure minibatches\n",
    "                        (minibatch_X, minibatch_Y) = minibatch\n",
    "                        # optimize for cost, \n",
    "                        _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: tf.one_hot(minibatch_Y, 1000)})\n",
    "                        # cumulative minibatch cost\n",
    "                        epoch_cost += minibatch_cost/num_minibatches\n",
    "            \n",
    "            # Print the cost after every 5 epochs\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Implement the YOLOv1 model\n",
    "\n",
    "- Implement Forward Propagation function\n",
    "- Implement cost function\n",
    "- Implement model function\n",
    "\n",
    "Create placeholders for the feature and label tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test script for the create-placeholder function\n",
    "X, Y = create_placeholders(448, 448, 3, 1000)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare and initialize the parameters that are used in the model. Traditional implementation of a CNN would have had to initialize them randomly. But the YOLOv1 model is pre-trained on ImageNet. These weights can be procured from Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Yet to be coded\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the YOLOv1 CNN forward propogation function. Facts that you need to paid attention\n",
    "- Linear activation for the final layer, leaky relu for the rest with alpha = 0.1\n",
    "- Any image is resized to 448x448. This is the standard input.\n",
    "- Implement a function to load filter dimensions from an xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# experimenting with the ElementTree API\n",
    "tf.reset_default_graph()\n",
    "parameters = {}\n",
    "tree = ET.parse('Configuration.xml')\n",
    "root = tree.getroot()\n",
    "for child in root:\n",
    "    size = []\n",
    "    for child1 in child:\n",
    "        # print(child.attrib['name'], child1.tag, child1.text)\n",
    "        if (child1.tag == 'dimension'):\n",
    "            size.append((int)(child1.text))\n",
    "            size.append((int)(child1.text))\n",
    "        if (child1.tag == 'input'):\n",
    "            size.append((int)(child1.text))\n",
    "        if (child1.tag == 'output'):\n",
    "            size.append((int)(child1.text))\n",
    "    print(size)\n",
    "    W = tf.get_variable(child.attrib['name'], size, initializer = tf.contrib.layers.xavier_initializer(seed = 0)) \n",
    "    parameters[child.attrib['name']] = W\n",
    "    B = tf.Variable(tf.constant(0.01, shape=[size[-1]]))\n",
    "    parameters['B'+(child.attrib['name'][1:])] = B\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in parameters:\n",
    "    print(key, parameters[key])\n",
    "    print(parameters[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_initialize_weights(xml_file):\n",
    "    '''\n",
    "    Reads model parameter weights from xml_file and initializes filters and biases\n",
    "    \n",
    "    Args:\n",
    "    xml_file - configuration xml with absolute path\n",
    "    \n",
    "    Returns:\n",
    "    parameters - a dictionary containing initialized parameters\n",
    "    '''\n",
    "    parameters = {}\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        size = []\n",
    "        for child1 in child:\n",
    "            # print(child.attrib['name'], child1.tag, child1.text)\n",
    "            if (child1.tag == 'dimension'):\n",
    "                size.append((int)(child1.text))\n",
    "                size.append((int)(child1.text))\n",
    "            if (child1.tag == 'input'):\n",
    "                size.append((int)(child1.text))\n",
    "            if (child1.tag == 'output'):\n",
    "                size.append((int)(child1.text))\n",
    "        print(size)\n",
    "        W = tf.get_variable(child.attrib['name'], size, initializer = tf.contrib.layers.xavier_initializer(seed = 0)) \n",
    "        parameters[child.attrib['name']] = W\n",
    "        B = tf.Variable(tf.constant(0.01, shape=[size[-1]]))\n",
    "        parameters['B'+(child.attrib['name'][1:])] = B\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_YOLOv1(X, parameters):\n",
    "    '''\n",
    "    Args:\n",
    "    X - placeholder for the initial feature tensor\n",
    "    parameters - dictionary containing filters\n",
    "    \n",
    "    returns\n",
    "    Z8 - output of the last LINEAR layer\n",
    "    \n",
    "    NOT IMPLEMENTED: NORMALIZATION\n",
    "    '''\n",
    "    \n",
    "    Z1 = tf.nn.conv2d(X, parameters['W01'], [1,2,2,1], padding=\"VALID\")\n",
    "    Z1 = tf.nn.bias_add(Z1, parameters['B01'])\n",
    "    A1 = tf.nn.leaky_relu(Z1, alpha=0.1)\n",
    "    P1 = tf.nn.max_pool(A1, [1,2,2,1], [1,2,2,1], padding=\"VALID\")\n",
    "    \n",
    "    \n",
    "    Z2 = tf.nn.conv2d(P1, parameters['W02'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z2 = tf.nn.bias_add(Z2, parameters['B02'])\n",
    "    A2 = tf.nn.leaky_relu(Z2, alpha=0.1)\n",
    "    P2 = tf.nn.max_pool(A2, [1,2,2,1], [1,2,2,1], padding=\"VALID\")\n",
    "    \n",
    "    Z3 = tf.nn.conv2d(P2, parameters['W03'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z3 = tf.nn.bias_add(Z3, parameters['B03'])\n",
    "    A3 = tf.nn.leaky_relu(Z3, alpha=0.1)\n",
    "    Z4 = tf.nn.conv2d(A3, parameters['W04'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z4 = tf.nn.bias_add(Z4, parameters['B04'])\n",
    "    A4 = tf.nn.leaky_relu(Z4, alpha=0.1)\n",
    "    Z5 = tf.nn.conv2d(A4, parameters['W05'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z5 = tf.nn.bias_add(Z5, parameters['B05'])\n",
    "    A5 = tf.nn.leaky_relu(Z5, alpha=0.1)\n",
    "    Z6 = tf.nn.conv2d(A5, parameters['W06'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z6 = tf.nn.bias_add(Z6, parameters['B06'])\n",
    "    A6 = tf.nn.leaky_relu(Z6, alpha=0.1)\n",
    "    P3 = tf.nn.max_pool(A6, [1,2,2,1], [1,2,2,1], padding=\"VALID\")\n",
    "    \n",
    "    Z7 = tf.nn.conv2d(P3, parameters['W07'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z7 = tf.nn.bias_add(Z7, parameters['B07'])\n",
    "    A7 = tf.nn.leaky_relu(Z7, alpha=0.1)\n",
    "    Z8 = tf.nn.conv2d(A7, parameters['W08'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z8 = tf.nn.bias_add(Z8, parameters['B08'])\n",
    "    A8 = tf.nn.leaky_relu(Z8, alpha=0.1)\n",
    "    Z9 = tf.nn.conv2d(A8, parameters['W09'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z9 = tf.nn.bias_add(Z9, parameters['B09'])\n",
    "    A9 = tf.nn.leaky_relu(Z9, alpha=0.1)\n",
    "    Z10 = tf.nn.conv2d(A9, parameters['W10'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z10 = tf.nn.bias_add(Z10, parameters['B10'])\n",
    "    A10 = tf.nn.leaky_relu(Z10, alpha=0.1)\n",
    "    Z11 = tf.nn.conv2d(P10, parameters['W11'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z11 = tf.nn.bias_add(Z11, parameters['B11'])\n",
    "    A11 = tf.nn.leaky_relu(Z11, alpha=0.1)\n",
    "    Z12 = tf.nn.conv2d(A11, parameters['W12'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z12 = tf.nn.bias_add(Z12, parameters['B12'])\n",
    "    A12 = tf.nn.leaky_relu(Z12, alpha=0.1)\n",
    "    Z13 = tf.nn.conv2d(A12, parameters['W13'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z13 = tf.nn.bias_add(Z13, parameters['B13'])\n",
    "    A13 = tf.nn.leaky_relu(Z13, alpha=0.1)\n",
    "    Z14 = tf.nn.conv2d(A13, parameters['W14'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z14 = tf.nn.bias_add(Z14, parameters['B14'])\n",
    "    A14 = tf.nn.leaky_relu(Z14, alpha=0.1)\n",
    "    Z15 = tf.nn.conv2d(P4, parameters['W15'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z15 = tf.nn.bias_add(Z15, parameters['B15'])\n",
    "    A15 = tf.nn.leaky_relu(Z15, alpha=0.1)\n",
    "    Z16 = tf.nn.conv2d(A15, parameters['W16'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z16 = tf.nn.bias_add(Z16, parameters['B16'])\n",
    "    A16 = tf.nn.leaky_relu(Z16, alpha=0.1)\n",
    "    P4 = tf.nn.max_pool(A16, [1,2,2,1], [1,2,2,1], padding=\"VALID\")\n",
    "    \n",
    "    \n",
    "    Z17 = tf.nn.conv2d(P4, parameters['W17'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z17 = tf.nn.bias_add(Z17, parameters['B17'])\n",
    "    A17 = tf.nn.leaky_relu(Z17, alpha=0.1)\n",
    "    Z18 = tf.nn.conv2d(A17, parameters['W18'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z18 = tf.nn.bias_add(Z18, parameters['B18'])\n",
    "    A18 = tf.nn.leaky_relu(Z18, alpha=0.1)\n",
    "    Z19 = tf.nn.conv2d(A18, parameters['W19'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z19 = tf.nn.bias_add(Z19, parameters['B19'])\n",
    "    A19 = tf.nn.leaky_relu(Z19, alpha=0.1)\n",
    "    Z20 = tf.nn.conv2d(A19, parameters['W20'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z20 = tf.nn.bias_add(Z20, parameters['B20'])\n",
    "    A20 = tf.nn.leaky_relu(Z20, alpha=0.1)\n",
    "    Z21 = tf.nn.conv2d(A20, parameters['W21'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z21 = tf.nn.bias_add(Z21, parameters['B21'])\n",
    "    A21 = tf.nn.leaky_relu(Z21, alpha=0.1)\n",
    "    Z22 = tf.nn.conv2d(A20, parameters['W22'], [1,2,2,1], padding=\"VALID\")\n",
    "    Z22 = tf.nn.bias_add(Z22, parameters['B22'])\n",
    "    A22 = tf.nn.leaky_relu(Z22, alpha=0.1)\n",
    "    \n",
    "    Z23 = tf.nn.conv2d(A22, parameters['W23'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z23 = tf.nn.bias_add(Z23, parameters['B23'])\n",
    "    A23 = tf.nn.leaky_relu(Z23, alpha=0.1)\n",
    "    Z24 = tf.nn.conv2d(A23, parameters['W24'], [1,1,1,1], padding=\"VALID\")\n",
    "    Z24 = tf.nn.bias_add(Z24, parameters['B24'])\n",
    "    A24 = tf.nn.leaky_relu(Z24, alpha=0.1)\n",
    "    \n",
    "    A24 = tf.contrib.layers.flatten(A24)\n",
    "    FC1 = tf.contrib.layers.fully_connected(A24, 512, activation_fn=None)\n",
    "    FC2 = tf.contrib.layers.fully_connected(FC1, 4096, activation_fn=None)\n",
    "    FC3 = tf.contrib.layers.fully_connected(FC2, 1470, activation_fn=None)\n",
    "    \n",
    "    return FC3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function\n",
    "The cost function is slightly tricky in YOLOv1. The model is optimized end-to-end and has a composite loss function. The cost function has been coded for an output tensor of shape 7x7x(2x5 + 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_YOLOv1_cost(y_pred, y_ground):\n",
    "    '''\n",
    "    Calculates the loss for gradient descent\n",
    "    \n",
    "    y_pred - predicted values - a 7x7x(2x5 + 20) tensor\n",
    "    y_ground - ground truth labels - a 7x7x(2x5 + 20) tensor\n",
    "    '''\n",
    "    predictedBoxScores = np.reshape(y_pred, [-1, 7, 7, 30])\n",
    "    predictedClasses = predictedBoxScores[:, :, :, :20]\n",
    "    predictedObjectConfidence = predictedBoxScores[:, :, :, 20:22]\n",
    "    predictedBoxDimensions = predictedBoxScores[:, :, :, 22:]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
