{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet Dataset Exploration\n",
    "\n",
    "This notebook aims to explore and prep the ImageNet(IN) dataset for training on YOLOv1 model. The model is built on the Tensorflow framework. The IN dataset can be downloaded from [here](http://image-net.org/download). Download and extract it. The path to the home directory(ILSVRC) should be stored in the following variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_ImageNet_path = \"/home/aditya/Documents/public_data/ILSVRC\"\n",
    "classification_path = absolute_ImageNet_path + \"/Data/CLS-LOC\"\n",
    "classification_training_path = classification_path + \"/train/\"\n",
    "classification_validation_path = classification_path + \"/val/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things that this notbook tries to achieve\n",
    "- Explore, visualize and learn how the data and the corresponding labels are structured.\n",
    "- Try and device an efficient way to load data at runtime for training. For obvious reasons(Limited CPU), we cannot work with hdf5 file formats or Tensor records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import misc\n",
    "import sys\n",
    "import skimage\n",
    "from skimage import transform\n",
    "from skimage.viewer import ImageViewer\n",
    "import h5py\n",
    "import json\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folders = os.listdir(absolute_ImageNet_path)\n",
    "classification_folders = os.listdir(classification_path)\n",
    "train_folder = os.listdir(classification_path+\"/train\")\n",
    "val_folder = os.listdir(classification_path+\"/val\")\n",
    "test_folder = os.listdir(classification_path+\"/test\")\n",
    "print(len(main_folders))\n",
    "print(len(classification_folders), len(train_folder), len(val_folder), len(test_folder))\n",
    "sorted_train_folder = sorted(train_folder)\n",
    "\n",
    "reference_dictionary = {}\n",
    "\n",
    "# code to convert file names to numbers for ease of data-processing\n",
    "n = 0\n",
    "for folder in sorted_train_folder:\n",
    "    reference_dictionary[folder] = n\n",
    "    n += 1\n",
    "    \n",
    "# with open('./ImageNet_dataset/class_coding.txt', 'w') as file:\n",
    "#     file.write(json.dumps(reference_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = classification_path+\"/train/\"+train_folder[1]\n",
    "images = os.listdir(path)\n",
    "\n",
    "# code to display an image\n",
    "im = Image.open(path+'/'+images[19])\n",
    "#im.show()\n",
    "\n",
    "# code to resize the image into 224x224\n",
    "plt.imshow(skimage.transform.resize((np.asarray(im)), (224, 224)))\n",
    "\n",
    "a = np.asarray(im)[None]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test script : DELETE\n",
    "im.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main interest is with the classification folder under Data. Creating hd5 files of different sizes for train/val/test folders would be a good way to go about it. The main problem is to replicate randomness in the way it is stored. We dont want images of the same class to be saved in a sequence.\n",
    "\n",
    "Let us create 130 h5 files, each file containing 10,000 images. The distribution has to be kept homogenous.\n",
    "\n",
    "steps involved:\n",
    "- check for different image modes and format in the dataset, convert to RGB if different\n",
    "- resize the images to 224x224\n",
    "- create separate homogenous lists of image and labelled data\n",
    "- shuffle the lists to make them random and distributed for the model to be trained on\n",
    "- create multiple h5 files to load dynamically while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check whether all train folders have 1300 images\n",
    "# print out anomolous folders and the number of images they contain\n",
    "anomolous_folders = []\n",
    "for folder in train_folder:\n",
    "    if (len(os.listdir(classification_path+\"/train/\"+folder)) == 1300):\n",
    "        continue\n",
    "    else:\n",
    "        print(folder)\n",
    "        anomolous_folders.append(folder)\n",
    "print(len(anomolous_folders))\n",
    "len(os.listdir(classification_path+\"/train/\"+anomolous_folders[104]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test script : DELETE\n",
    "image_files = os.listdir(classification_training_path+train_folder[0])\n",
    "image = Image.open(classification_training_path+train_folder[0]+'/'+image_files[2])\n",
    "print(image.mode)\n",
    "image.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell checks all the images in the dataset for the stored format. The model we train needs to have a fixed dimension of images - 224x224x3. There are a few images in the dataset that are of different dimensions. Lets get to the bottom of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN THIS CELL\n",
    "# code to check all the different image formats and dimensions.\n",
    "image_mode_dict = {}\n",
    "for folder in train_folder:\n",
    "    temp_path = classification_training_path+folder\n",
    "    image_files = os.listdir(temp_path)\n",
    "    for image_file in image_files:\n",
    "        try:\n",
    "            image = Image.open(temp_path + '/' + image_file)\n",
    "            if image.mode in image_mode_dict:\n",
    "                image_mode_dict[image.mode] += 1\n",
    "            else:\n",
    "                image_mode_dict[image.mode] = 1\n",
    "                \n",
    "        finally:\n",
    "            image.close()\n",
    "print(image_mode_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell implements all the steps mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test script: DELETE\n",
    "h5file_path = \"./ImageNet_dataset/training_folder/train_test_file.hdf5\"\n",
    "\n",
    "image_data = []\n",
    "label_data = []\n",
    "\n",
    "test_path = classification_training_path+train_folder[0]\n",
    "image_files = os.listdir(test_path)\n",
    "for image_file in image_files:\n",
    "    # code to check the number of channels of each image loaded and resize appropriately\n",
    "    image = Image.open(test_path+'/'+image_file)\n",
    "    if image.mode == 'L':\n",
    "        rgbimage = Image.new(\"RGB\", image.size)\n",
    "        rgbimage.paste(image)\n",
    "        resized_image = skimage.transform.resize((np.asarray(rgbimage)), (224, 224))\n",
    "        image_data.append(resized_image)\n",
    "        label_data.append(reference_dictionary[folder])\n",
    "        rgbimage.close()\n",
    "    elif image.mode == 'RGB':\n",
    "        resized_image = skimage.transform.resize((np.asarray(image)), (224, 224))\n",
    "        image_data.append(resized_image)\n",
    "        label_data.append(reference_dictionary[folder])\n",
    "    elif image.mode == 'RGBA':\n",
    "        image_array = np.asarray(image)\n",
    "        resized_image = skimage.transform.resize(image_array[:, :, :3], (224, 224))\n",
    "        image_data.append(resized_image)\n",
    "        label_data.append(reference_dictionary[folder])\n",
    "    image.close()\n",
    "    \n",
    "# initializing the h5 file\n",
    "X_dataset_shape = (len(image_data), 224, 224, 3)\n",
    "Y_dataset_shape = (len(image_data),)\n",
    "h5file = h5py.File(h5file_path, mode = 'w')\n",
    "h5file.create_dataset(\"X_train\", X_dataset_shape, np.float32)\n",
    "h5file.create_dataset(\"Y_train\", Y_dataset_shape, np.uint16)\n",
    "\n",
    "# shuffling the data\n",
    "c = list(zip(image_data, label_data))\n",
    "shuffle(c)\n",
    "image_data, label_data = zip(*c)\n",
    "\n",
    "#plt.imshow(image_data[0])\n",
    "print(len(image_data))\n",
    "print(len(label_data))\n",
    "\n",
    "# storing the data in the h5 file\n",
    "length_image_data = len(image_data)\n",
    "h5file[\"X_train\"][...] = image_data\n",
    "h5file[\"Y_train\"][...] = label_data\n",
    "\n",
    "plt.imshow(h5file['X_train'][0])\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test script: DELETE\n",
    "h5_path = \"./ImageNet_dataset/training_folder/train_test_file.hdf5\"\n",
    "h5_file = h5py.File(h5_path, mode = 'r')\n",
    "total_num_samples = h5_file[\"X_train\"].shape\n",
    "total_labels = h5_file[\"Y_train\"].shape\n",
    "print(total_num_samples, total_labels)\n",
    "\n",
    "image = h5_file['X_train'][0]\n",
    "plt.imshow(image)\n",
    "print(image)\n",
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Training h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code to resize and store the images in a h5 file.\n",
    "n = 0\n",
    "count = 0\n",
    "folders_skipped = 0\n",
    "sample = 2\n",
    "while folders_skipped != 1000:\n",
    "        \n",
    "    # code to initialize a h5 file\n",
    "    h5file_path = \"./ImageNet_dataset/training_folder/train\"+str(count)+\".hdf5\"\n",
    "\n",
    "    # code to initialize a global variable to store the image data\n",
    "    image_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    # variable to keep a track of number of skipped folders\n",
    "    folders_skipped = 0\n",
    "\n",
    "    # code to recursively go through each folder and select 10 images to store\n",
    "    for folder in train_folder:\n",
    "        temp_path = classification_training_path+folder\n",
    "        image_files = os.listdir(temp_path)\n",
    "        if (n >= len(image_files)):\n",
    "            folders_skipped += 1\n",
    "            continue\n",
    "        elif ((len(image_files) - 1) - n < sample):\n",
    "            concerned_image_files = image_files[n:]\n",
    "        else:\n",
    "            concerned_image_files = image_files[n:n+sample]\n",
    "        for image_file in concerned_image_files:\n",
    "\n",
    "            # code to check the number of channels of each image loaded and resize appropriately\n",
    "            image = Image.open(temp_path+'/'+image_file)\n",
    "            if image.mode == 'L':\n",
    "                rgbimage = Image.new(\"RGB\", image.size)\n",
    "                rgbimage.paste(image)\n",
    "                resized_image = skimage.transform.resize((np.asarray(rgbimage)), (224, 224))\n",
    "                image_data.append(resized_image)\n",
    "                label_data.append(reference_dictionary[folder])\n",
    "                rgbimage.close()\n",
    "            elif image.mode == 'RGB':\n",
    "                resized_image = skimage.transform.resize((np.asarray(image)), (224, 224))\n",
    "                image_data.append(resized_image)\n",
    "                label_data.append(reference_dictionary[folder])\n",
    "            elif image.mode == 'RGBA':\n",
    "                image_array = np.asarray(image)\n",
    "                resized_image = skimage.transform.resize(image_array[:, :, :3], (224, 224))\n",
    "                image_data.append(resized_image)\n",
    "                label_data.append(reference_dictionary[folder])\n",
    "            image.close()\n",
    "\n",
    "    # increment n\n",
    "    n += sample\n",
    "\n",
    "    # initializing the h5 file\n",
    "    X_dataset_shape = (len(image_data), 224, 224, 3)\n",
    "    Y_dataset_shape = (len(image_data),)\n",
    "    h5file = h5py.File(h5file_path, mode = 'w')\n",
    "    h5file.create_dataset(\"X_train\", X_dataset_shape, np.float16)\n",
    "    h5file.create_dataset(\"Y_train\", Y_dataset_shape, np.uint16)\n",
    "\n",
    "    # shuffling the data\n",
    "    c = list(zip(image_data, label_data))\n",
    "    shuffle(c)\n",
    "    image_data, label_data = zip(*c)\n",
    "    \n",
    "    print(len(image_data))\n",
    "    print(len(label_data))\n",
    "\n",
    "    # storing the data in the h5 file\n",
    "    h5file[\"X_train\"][...] = image_data\n",
    "    h5file[\"Y_train\"][...] = label_data\n",
    "    h5file.close()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.mode)\n",
    "image_array = np.asarray(image)\n",
    "image_array.shape\n",
    "print(n)\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below checks whether the h5 files are accurate by displaying a random image and printing the corresponding label. We can match the label against the semantic names of individual class codes [here](https://github.com/Lasagne/Recipes/blob/master/examples/resnet50/imagenet_classes.txt).(don't forget to add 1 to the coding as it is 0 indexed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to check h5 files\n",
    "h5_path = \"./ImageNet_dataset/training_folder/train\"+str(600)+\".hdf5\"\n",
    "\n",
    "\n",
    "with h5py.File(h5_path, mode = 'r') as h5_file:\n",
    "    total_num_samples = h5_file[\"X_train\"].shape[0]\n",
    "    total_labels = h5_file[\"Y_train\"].shape[0]\n",
    "    print(total_num_samples, total_labels)\n",
    "\n",
    "    image = h5_file['X_train'][500]\n",
    "    label = h5_file['Y_train'][500]\n",
    "    # viewer = ImageViewer(image)\n",
    "    # viewer.show()\n",
    "    # print(image)\n",
    "    for classes, num in reference_dictionary.items():\n",
    "        if num == label:\n",
    "            print(num)\n",
    "    misc.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets experiment with the tf.one_hot to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(h5_path, mode = 'r') as h5_file:\n",
    "    Y_train = np.asarray(h5_file['Y_train'])\n",
    "    print(Y_train.shape)\n",
    "    print(np.asarray(tf.one_hot(Y_train, 1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Validation h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def parse_validation_xml(xml_file):\n",
    "    \"\"\"\n",
    "    absolute path of ImageNet validation xml file\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        if (child.tag == 'object'):\n",
    "            for sub_child in child:\n",
    "                if (sub_child.tag == 'name'):\n",
    "                    result = sub_child.text\n",
    "     \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # code to initialize a h5 file\n",
    "    H5FILE_VAL_PATH = \"/media/aditya/Seagate Backup Plus Drive/h5_FILES_Aditya/validation_folder/val.hdf5\"\n",
    "    ANN_PATH = '/home/aditya/Documents/public_data/ILSVRC/Annotations/CLS-LOC/val/' \n",
    "\n",
    "\n",
    "\n",
    "    # code to initialize a global variable to store the image data\n",
    "    image_data = []\n",
    "    label_data = []\n",
    "\n",
    "    # image files\n",
    "    image_files = os.listdir(classification_validation_path)\n",
    "    length = len(image_files)\n",
    "\n",
    "    # initializing the h5 file\n",
    "    X_dataset_shape = (length, 224, 224, 3)\n",
    "    Y_dataset_shape = (length,)\n",
    "    h5file = h5py.File(H5FILE_VAL_PATH, mode = 'w')\n",
    "    h5file.create_dataset(\"X_val\", X_dataset_shape, np.float16)\n",
    "    h5file.create_dataset(\"Y_val\", Y_dataset_shape, np.uint16)\n",
    "\n",
    "    for i in range(length):\n",
    "\n",
    "        # code to retrieve/construct the validation annotation string\n",
    "        image_class = image_files[i].split('.')\n",
    "        xml_file = image_class[0]+'.xml'\n",
    "\n",
    "        # code to check the number of channels of each image loaded and resize appropriately\n",
    "        image = Image.open(classification_validation_path+image_files[i])\n",
    "        if image.mode == 'L':\n",
    "            rgbimage = Image.new(\"RGB\", image.size)\n",
    "            rgbimage.paste(image)\n",
    "            resized_image = skimage.transform.resize((np.asarray(rgbimage)), (224, 224))\n",
    "\n",
    "            # save the image and label data\n",
    "            h5file[\"X_val\"][i, ...] = resized_image[None]\n",
    "            label_data.append(reference_dictionary[parse_validation_xml(ANN_PATH+xml_file)])\n",
    "            rgbimage.close()\n",
    "        elif image.mode == 'RGB':\n",
    "            resized_image = skimage.transform.resize((np.asarray(image)), (224, 224))\n",
    "            h5file[\"X_val\"][i, ...] = resized_image[None]\n",
    "            label_data.append(reference_dictionary[parse_validation_xml(ANN_PATH+xml_file)])\n",
    "        elif image.mode == 'RGBA':\n",
    "            image_array = np.asarray(image)\n",
    "            resized_image = skimage.transform.resize(image_array[:, :, :3], (224, 224))\n",
    "            h5file[\"X_val\"][i, ...] = resized_image[None]\n",
    "            label_data.append(reference_dictionary[parse_validation_xml(ANN_PATH+xml_file)])\n",
    "        image.close()\n",
    "        print(image_files[i])\n",
    "\n",
    "    \n",
    "\n",
    "    # storing the data in the h5 file\n",
    "    h5file[\"Y_val\"][...] = label_data\n",
    "finally:\n",
    "    image.close()\n",
    "    h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.close()\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the validation annotations\n",
    "ANN_PATH = '/home/aditya/Documents/public_data/ILSVRC/Annotations/CLS-LOC/val/'\n",
    "xml_files = os.listdir(ANN_PATH)\n",
    "print(xml_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_validation_xml(ANN_PATH+xml_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That marks the end of that. The tests were good, and the ImageNet dataset is ready to be trained upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
